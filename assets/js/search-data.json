{
  
    
        "post0": {
            "title": "An Overview of Initialization for Deep Neural Networks, Part 1",
            "content": "For the past few years deep neural networks have proven themselves to be able to solve various AI-level tasks effectively. In some very specific tasks, such as image classification, game of Go, autonomous driving, and many others they already perform better than humans. These achievements were not possible without tremendous research invested in study of difficulties that occur during training of deep networks, and methods to overcome them. A part of this progress is due to better non-linearities and initialization methods. . What Is the Choice of Activation Function and Initialization and Why Is It Important? . Neural networks are typically trained via stochastic gradient descent or one of its more powerful variants, such as RMSprop or Adam, that use gradients computed by the back propagation algorithm. The back propagation algorithm essentially recursively applies the chain rule of calculus to compute gradients efficiently. Just as a reminder, the chain rule of calculus look like this . begin{equation} frac{ partial}{ partial x}g(f(x)) = frac{ partial g}{ partial f} frac{ partial f}{ partial x} end{equation}In earlier years folks used sigmoid activation function, defined as . begin{equation} sigma(x) = frac{1}{1 + e^{-x}} end{equation}However, soon it became evident that such nets are hard to train, because of some properties of sigmoid. One such property is the effect of saturation. Look at the graph of sigmoid. . def sigmoid(x): return 1. / (1. + np.exp(-x)) def sigmoid_derivative(x): return sigmoid(x) * (1. - sigmoid(x)) def plot_activation_and_derivative(activation, derivative, inputs, titles): fig, (ax_f, ax_d) = plt.subplots(1, 2, sharey=True, figsize=(16, 5)) ax_f.plot(inputs, activation(inputs)) ax_f.set_title(titles[0]) ax_d.plot(inputs, derivative(inputs)) ax_d.set_title(titles[1]) plt.show() values = np.arange(-6.0, 6.0, 0.01) plot_activation_and_derivative(sigmoid, sigmoid_derivative, values, (&#39;Sigmoid activation&#39;, &#39;Derivative of sigmoid&#39;)) . When input values become large enough (whether positive or negative), output of sigmoid remains roughly unchanged and goes to 1 in case of positive inputs, or to 0 in case of negative. This saturation of sigmoid causes gradient vanishing. Since output values are roughly equal in those saturated tails, the derivative of sigmoid becomes very close to zero. As back propagation progresses towards input layers, gradients get multiplied, and if at least one of multiples becomes close to zero, the whole product is likely to become close to zero as well. Hence it can require too many gradient updates to tweak parameters in right way. Now look at the graph of the derivative of sigmoid more closely. You can note that its maximum value is 0.25 when input value is 0. So, in the best case, gradient gets four times smaller after sigmoid activation during back propagation. . There is another saturating activation function that, however, suffers less from vanishing gradient problems. It is called hyperbolic tangent, or tanh for short. . begin{equation} tanh(x) = frac{e^x - e^{-x}}{e^x + e^{-x}} end{equation}Actually it can be shown from their definitions that tanh can be expressed in terms of sigmoid as follows . begin{equation} tanh(x) = 2 sigma(2x) - 1 end{equation}In spite of that, tanh has two advantages over the sigmoid. First, it is symmetric in the origin, which makes it easier to learn identity function, and second, maximum value of its derivative is 1. . def tanh(x): return np.tanh(x) def tanh_derivative(x): return 1. / np.cosh(x) ** 2 plot_activation_and_derivative(tanh, tanh_derivative, values, (&#39;Tanh activation&#39;, &#39;Tanh derivative&#39;)) . Tanh is widely used in recurrent neural networks today. . Nowadays we have many ways to address vanishing gradient and similar exploding gradient issues. And one of these is better initialization. Initialization is also important because gradient-based methods can stuck in bad local minima, not reaching the global minimum, and take long to converge because of plateus in the cost function surface. Better initialization reduces chances to start from &quot;unlucky&quot; points. . LeCun Initialization . This is one of the first heuristics for initialization. It was proposed in 1998 by Yann LeCun et al. in the paper called &quot;Efficient BackProp&quot;. LeCun initialization suggests that weights of network are drawn from normal distribution of the following form . begin{equation} W sim mathcal{N} Bigg(0, frac{1}{fan_{in}} Bigg) end{equation}where $fan_{in}$ is the number of connections feeding into the node. This is also known as LeCun normal initialization, while in case of LeCun uniform initialization weights are drawn from uniform distribution . begin{equation} W sim mathcal{U} Bigg(- frac{1}{ sqrt{fan_{in}}}, frac{1}{ sqrt{fan_{in}}} Bigg) end{equation}LeCun initialization was designed specifically for sigmoid activation. But it has more applications than that. It was proposed to be used with the SELU activation function and self-normalizing networks. . Note that regardless the method of initialization, weights are usually drawn either from normal or uniform distribution, and there&#39;s connection between them. If we initialize from a normal distribution with $ mathrm{Var}[W] = sigma^2$, we can easily switch to uniform distribution by computing $a = sqrt{3 sigma^2}$ and sampling weights from $ mathcal{U}(-a, a)$. Because of this connection I will only consider normal distribution and specify suggested variance for it. . Derivation of LeCun Initialization . The reasoning behind LeCun initialization is fairly simple. We don&#39;t want weights to be very large, because they will lead to saturation and cause gradient vanishing. We also don&#39;t want weights to be very small, because they will make gradient small as well and make the whole optimization slow. What we want is that initial weights map inputs into linear region of sigmoid (roughly between -2 and 2), so the gradient is large enough and network learns easier linear part first. . Consider a sigmoid layer and let $x$ denote input example and $y$ linearly transformed input, so . begin{equation} y = xW y_i = sum_{j = 1}^{fan_{in}}{x_{j}w_{ji}} end{equation}We can think of bias term as weight that always receives 1 as input and omit it. To achieve the above requirements we first demand $x$ to have zero mean and variance of 1. This is satisfied by normalizing inputs. Now we want initialize weights so that they ensure $y$ also has zero mean and unit variance. Let&#39;s write down expression for the variance of $y_i$ . begin{equation} mathrm{Var}[y_i] = mathrm{Var} Bigg[ sum_{j = 1}^{fan_{in}}{x_{j}w_{ji}} Bigg] = sum_{j = 1}^{fan_{in}} w_{ji}^2 mathrm{Var}[x_j] + sum_{j neq k}w_{ji}w_{ki} mathrm{Cov}[x_j, x_k] end{equation}Assuming uncorrelated $x$, we have $ mathrm{Cov}[x_j, x_k] = 0$ for all pairs $j neq k$. Recall we assumed all $ mathrm{Var}[x_i] = 1$. Hence . begin{equation} mathrm{Var}[y_i] = sum_{j = 1}^{fan_{in}} w_{ji}^2 Rightarrow mathrm{Var}[w] = frac{ mathrm{Var}[y]}{fan_{in}} = frac{1}{fan_{in}} end{equation}And that&#39;s it! One thing that should be mentioned here is that original paper also proposed a variant of sigmoid that ensures $ sigma(y)$ preserves zero mean and unit variance of $y$. . Xavier (Glorot) Initialization . The next method was proposed in paper &quot;Understanding the Difficulty of Training Deep Feedforward Neural Networks&quot; by Xavier Glorot and Yoshua Bengio in 2010. I don&#39;t know who decides whether we should call an invention after author&#39;s first name or last name, but this technique is widely known under both Xavier initialization and Glorot initialization. . If you have skimmed through the derivation of LeCun initialization above or read it in original paper, you might have noticed that it was motivated entirely from the perspective of the forward pass. In their paper Glorot and Bengio additionly require preserving variance of gradient during the backward pass. From one hand, the forward pass requires, as previously . begin{equation} mathrm{Var}[W] = frac{1}{fan_{in}} end{equation}They show that the backward pass from the other hand requires . begin{equation} mathrm{Var}[W] = frac{1}{fan_{out}} end{equation}where $fan_{out}$ is the number of connections going out of a unit. . Both constraints can be satisfied if and only if $fan_{in} = fan_{out}$, which isn&#39;t always the case. As a compromise authors suggest to set variance to . begin{equation} mathrm{Var}[W] = frac{1}{fan_{avg}} = frac{2}{fan_{in} + fan_{out}} end{equation}Observe also that when $fan_{in} = fan_{out}$ Xavier initialization and LeCun initialization are equivalent. . The paper compares performance of several networks of the same architecture that only differ in activation function and initialization method. Those networks were trained on different data sets using the same procedure. The final test errors for networks using tanh activation are represented in the following table . TYPE Shapeset MNIST CIFAR-10 Small ImageNet . Tanh | 27.15 | 1.76 | 55.9 | 70.58 | . Tanh N | 15.60 | 1.64 | 52.92 | 68.57 | . Here tanh stands for the network with tanh activations and LeCun uniform initialization, while tanh N - for the network with tanh activations and Xavier uniform initialization. As can be seen, on all data sets Xavier initialization gives little to significant boost in performance when compared to LeCun initialization. . A trouble with Xavier initialization is that it assumes linear units. This is a valid assumption when inputs are mapped to linear parts of non-linearities, but invalid in general. Nevertheless it still works pretty well. The following histograms of activation values and gradients from the paper show that Xavier initialization in experimental settings works as intended. Check out details in original paper! . Derivation of Xavier Initialization . Consider a neural network with $L$ consequent layers. We begin with writing down equations for both forward and backward propagations. We make the same assumptions about input data as in the derivation of LeCun initialization. The forward pass is given by . begin{equation} Z^{[i]}_k = varphi^{[i]}(Z^{[i - 1]}_k) W^{[i]}_{:, k} end{equation}And the backward pass by . begin{equation} frac{ partial mathscr{L}}{ partial Z^{[i]}_k} = varphi^{[i] prime}(Z^{[i]}_k)W^{[i]}_{k, :} frac{ partial mathscr{L}}{ partial Z^{[i + 1]}} frac{ partial mathscr{L}}{ partial W^{[i]}_{lk}} = Z^{[i]}_l frac{ partial mathscr{L}}{ partial Z^{[i]}_k} end{equation}Then for a layer number $i$, assuming that $ varphi^{[i] prime}(z^{[i]}_k) = 1$ . begin{equation} mathrm{Var}[Z^{[i]}] = mathrm{Var}[X] prod_{j=0}^{i-1}n^{[j]} mathrm{Var}[W^{[j]}] mathrm{Var} Bigg[ frac{ partial mathscr{L}}{ partial Z^{[i]}} Bigg] = mathrm{Var} Bigg[ frac{ partial mathscr{L}}{ partial Z^{[L]}} Bigg] prod_{j = i}^{L - 1}n^{[j + 1]} mathrm{Var}[W^{[j]}] mathrm{Var} Bigg[ frac{ partial mathscr{L}}{ partial W^{[i]}} Bigg] = mathrm{Var}[Z^{[i]}] mathrm{Var} Bigg[ frac{ partial mathscr{L}}{ partial Z^{[i]}} Bigg] end{equation}From the perspective of the forward pass we wish . begin{equation} forall (i, j): mathrm{Var}[Z^{[i]}] = mathrm{Var}[Z^{[j]}] end{equation}And from the back prop point of view we want . begin{equation} forall (i, j): mathrm{Var} Bigg[ frac{ partial mathscr{L}}{ partial Z^{[i]}} Bigg] = mathrm{Var} Bigg[ frac{ partial mathscr{L}}{ partial Z^{[j]}} Bigg] end{equation}These two constraints can be easily satisfied if . begin{equation} forall i: n^{[i]} mathrm{Var}[W^{[i]}] = mathrm{Var}[X] = 1 forall i: n^{[i + 1]} mathrm{Var} Bigg[ frac{ partial mathscr{L}}{ partial Z^{[i]}} Bigg] = mathrm{Var} Bigg[ frac{ partial mathscr{L}}{ partial Z^{[L]}} Bigg] = 1 end{equation}From these two equations we finally get . begin{equation} mathrm{Var}[W^{[i]}] = frac{1}{n^{[i]}} = frac{1}{fan_{in}} mathrm{Var}[W^{[i]}] = frac{1}{n^{[i + 1]}} = frac{1}{fan_{out}} end{equation}As a compromise for cases when $fan_{in} neq fan_{out}$ . begin{equation} mathrm{Var}[W^{[i]}] = frac{2}{fan_{in} + fan_{out}} end{equation} Conclusions . Without appropriate choice of activation function and/or smart initialization for it neural networks, especially deep ones, tend to refuse training at all. Better activation functions and better initialization strategies are tightly related, so that they complement each other and evolve together. Once popular sigmoid non-linearity turned out to be impractical to use in hidden layers, and now is only used in the output layer for binary classification, while tanh that can be expressed in terms of sigmoid, is still useful for hidden layers. At the first glance initialization strategies seem some wisdom sent from heaven, but at least in case of LeCun and Xavier initialization their derivations are in fact straightforward. All you need to know is where to start and what assumptions to make. .",
            "url": "https://akulchik.github.io/blog/fastpages/jupyter/2020/07/31/An-Overview-of-Activations-and-Initializations-for-Deep-Neural-Networks,-Part-1.html",
            "relUrl": "/fastpages/jupyter/2020/07/31/An-Overview-of-Activations-and-Initializations-for-Deep-Neural-Networks,-Part-1.html",
            "date": " • Jul 31, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://akulchik.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://akulchik.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://akulchik.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://akulchik.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}